{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b66c27-ae47-40f8-9761-8c0c205c3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp[kubernetes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0048f8-cd18-4613-b088-dc0586adc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp \n",
    "\n",
    "from kfp.dsl import component, pipeline\n",
    "from kfp import kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa3e16-4157-4433-9416-9d5404532864",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTP_PROXY, HTTPS_PROXY, NO_PROXY = 'http://egress.ps7.internal:3128', 'http://egress.ps7.internal:3128', \"127.0.0.1,localhost,::1,10.0.0.0/8,172.16.0.0/16,192.168.0.0/16,10.152.183.0/24,.svc,.local,.kubeflow\"\n",
    "\n",
    "def add_proxy(obj, http_proxy=HTTP_PROXY, https_proxy=HTTPS_PROXY, no_proxy=NO_PROXY):\n",
    "    \"\"\"Adds the proxy env vars to the PipelineTask object.\"\"\"\n",
    "    return (\n",
    "        obj.set_env_variable(name=\"http_proxy\", value=http_proxy)\n",
    "        .set_env_variable(name=\"https_proxy\", value=https_proxy)\n",
    "        .set_env_variable(name=\"HTTP_PROXY\", value=http_proxy)\n",
    "        .set_env_variable(name=\"HTTPS_PROXY\", value=https_proxy)\n",
    "        .set_env_variable(name=\"no_proxy\", value=no_proxy)\n",
    "        .set_env_variable(name=\"NO_PROXY\", value=no_proxy)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120b3e2-c59d-47f6-8543-ed5c9420bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as below, but some of the logic put in the spark8t library\n",
    "# @component(\n",
    "#     base_image=\"docker.io/bikalpadhakalcanonical/charmed-spark:5\",\n",
    "#     packages_to_install=[\"pyspark==3.4.2\"]\n",
    "# )\n",
    "# def spark_test_component() -> None:\n",
    "#     import logging\n",
    "#     from operator import add\n",
    "#     from spark8t.session import SparkSession\n",
    "         \n",
    "#     def count_vowels(text: str) -> int:\n",
    "#       count = 0\n",
    "#       for char in text:\n",
    "#         if char.lower() in \"aeiou\":\n",
    "#           count += 1\n",
    "#       return count\n",
    "\n",
    "#     lines = \"\"\"Canonical's Charmed Data Platform solution for Apache Spark runs Spark jobs on your Kubernetes cluster.\n",
    "#     You can get started right away with MicroK8s - the mightiest tiny Kubernetes distro around! \n",
    "#     The spark-client snap simplifies the setup process to run Spark jobs against your Kubernetes cluster. \n",
    "#     Spark on Kubernetes is a complex environment with many moving parts.\n",
    "#     Sometimes, small mistakes can take a lot of time to debug and figure out.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     with SparkSession(app_name=\"CountVowels\", namespace=\"admin\", username=\"spark\") as spark:\n",
    "#         n = spark.sparkContext.parallelize(lines.splitlines(), 2).map(count_vowels).reduce(add)\n",
    "#         logging.warning(f\"The number of vowels in the string is {n}\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"ghcr.io/canonical/charmed-spark:3.5-22.04_edge\",\n",
    ")\n",
    "def spark_test_component() -> None:\n",
    "    import logging\n",
    "    import os\n",
    "    import pyspark\n",
    "    import socket\n",
    "    from lightkube import Client\n",
    "    from operator import add\n",
    "    from spark8t.services import K8sServiceAccountRegistry\n",
    "    from spark8t.services import LightKube as LightKubeInterface\n",
    "    \n",
    "    def count_vowels(text: str) -> int:\n",
    "      count = 0\n",
    "      for char in text:\n",
    "        if char.lower() in \"aeiou\":\n",
    "          count += 1\n",
    "      return count\n",
    "\n",
    "    lines = \"\"\"Canonical's Charmed Data Platform solution for Apache Spark runs Spark jobs on your Kubernetes cluster.\n",
    "    You can get started right away with MicroK8s - the mightiest tiny Kubernetes distro around! \n",
    "    The spark-client snap simplifies the setup process to run Spark jobs against your Kubernetes cluster. \n",
    "    Spark on Kubernetes is a complex environment with many moving parts.\n",
    "    Sometimes, small mistakes can take a lot of time to debug and figure out.\n",
    "    \"\"\"\n",
    "\n",
    "    app_name = \"CountVowels\"\n",
    "    SPARK_SERVICE_ACCOUNT = os.environ[\"SPARK_SERVICE_ACCOUNT\"]\n",
    "    SPARK_NAMESPACE = os.environ[\"SPARK_NAMESPACE\"]\n",
    "\n",
    "    pod_ip = socket.gethostbyname(socket.gethostname())\n",
    "    k8s_master = Client().config.cluster.server\n",
    "    interface = LightKubeInterface(None, None)\n",
    "    registry = K8sServiceAccountRegistry(interface)\n",
    "\n",
    "    import re\n",
    "    from spark8t.utils import environ\n",
    "\n",
    "    host_parser = re.compile(\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/\\n]+)\")\n",
    "    rest_api_host = host_parser.match(k8s_master).groups()[0]\n",
    "\n",
    "    with environ(NO_PROXY=rest_api_host, no_proxy=rest_api_host):\n",
    "        spark_properties = registry.get(\n",
    "            f\"{SPARK_NAMESPACE}:{SPARK_SERVICE_ACCOUNT}\"\n",
    "        ).configurations.props | {\n",
    "            \"spark.driver.host\": pod_ip,\n",
    "        }\n",
    "\n",
    "    builder = pyspark.sql.SparkSession\\\n",
    "                    .builder\\\n",
    "                    .appName(app_name)\\\n",
    "                    .master(f\"k8s://{k8s_master}\")\n",
    "    for conf, val in spark_properties.items():\n",
    "        builder = builder.config(conf, val)\n",
    "    session = builder.getOrCreate()\n",
    "\n",
    "    n = session.sparkContext.parallelize(lines.splitlines(), 2).map(count_vowels).reduce(add)\n",
    "    logging.warning(f\"The number of vowels in the string is {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dd7e2-473a-4fe5-92f5-c09ab7538c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"spark-test-pipeline\")\n",
    "def spark_pipeline():\n",
    "    task = add_proxy(spark_test_component())\n",
    "    kubernetes.add_pod_label(\n",
    "        task,\n",
    "        label_key='access-spark-pipeline',\n",
    "        label_value='true',\n",
    "    )\n",
    "    # kubernetes.add_pod_annotation(\n",
    "    #     task,\n",
    "    #     annotation_key='traffic.sidecar.istio.io/excludeInboundPorts',\n",
    "    #     annotation_value='37371,6060',\n",
    "    # )\n",
    "    # kubernetes.add_pod_annotation(\n",
    "    #     task,\n",
    "    #     annotation_key='traffic.sidecar.istio.io/excludeOutboundPorts',\n",
    "    #     annotation_value='37371,6060',\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba725d3-975a-48c7-9ab1-11d320c2090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=kfp.Client()\n",
    "kfp.compiler.Compiler().compile(\n",
    "    spark_pipeline,\n",
    "    package_path=\"spark_test_pipeline.yaml\"\n",
    ")\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    spark_pipeline,\n",
    "    arguments={},\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748b933-4304-4ef7-8e11-d316a29c5d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
